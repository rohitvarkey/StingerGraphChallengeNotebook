{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition SBM Baseline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the Python script by\\n\\nAuthors: Steven Smith, Edward Kao\\nDate: 9 January 2017\\nInstallation: Python 2.7\\n\\nDescription: This Python script performs the baseline graph partition algorithm based on the degree-corrected stochastic block model.\\n\\nReferences:\\nPeixoto, Tiago P. \\\"Entropy of stochastic blockmodel ensembles.\\\" Physical Review E 85, no. 5 (2012): 056122.\\nPeixoto, Tiago P. \\\"Parsimonious module inference in large networks.\\\" Physical review letters 110, no. 14 (2013): 148701.\\nKarrer, Brian, and Mark EJ Newman. \\\"Stochastic blockmodels and community structure in networks.\\\" Physical Review E 83, no. 1 (2011): 016107.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# License\n",
    "\n",
    "#\n",
    "# Copyright 2017 MIT Lincoln Laboratory, Massachusetts Institute of Technology\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use these files except in compliance with\n",
    "# the License.\n",
    "#\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
    "# an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
    "# specific language governing permissions and limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Based on the Python script by\n",
    "\n",
    "Authors: Steven Smith, Edward Kao\n",
    "Date: 9 January 2017\n",
    "Installation: Python 2.7\n",
    "\n",
    "Description: This Python script performs the baseline graph partition algorithm based on the degree-corrected stochastic block model.\n",
    "\n",
    "References:\n",
    "Peixoto, Tiago P. \"Entropy of stochastic blockmodel ensembles.\" Physical Review E 85, no. 5 (2012): 056122.\n",
    "Peixoto, Tiago P. \"Parsimonious module inference in large networks.\" Physical review letters 110, no. 14 (2013): 148701.\n",
    "Karrer, Brian, and Mark EJ Newman. \"Stochastic blockmodels and community structure in networks.\" Physical Review E 83, no. 1 (2011): 016107.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LightGraphs\n",
    "import StatsBase: pweights, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Graph\n",
    "\n",
    "We load the graph into a LightGraphs graph. We do this by reading the edges to be added and adding them onto the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"data/streaming/emergingEdges/\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const INPUT_PATH = \"data/streaming/emergingEdges/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'load_graph! :: Union{Tuple{LightGraphs.SimpleGraphs.SimpleDiGraph,String,Int64}, Tuple{LightGraphs.SimpleGraphs.SimpleDiGraph,String}}' in module 'Main'.\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "load_graph!"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load graph given the base filename and the number of the streaming peice to be added onto a given graph.\n",
    "\"\"\"\n",
    "function load_graph!(g::DiGraph, filename::String, streaming_num::Int64=1)\n",
    "    info(\"Loading $(filename)_$(streaming_num).tsv\")\n",
    "    edgePieces = readdlm(\"$(filename)_$(streaming_num).tsv\")\n",
    "    for i = 1:size(edgePieces, 1)\n",
    "        # TODO: Add weights also\n",
    "        success = add_edge!(g, edgePieces[i, 1], edgePieces[i, 2])\n",
    "        if success == false\n",
    "            throw(\"Error adding edges.\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the edge counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'initalize_edge_counts :: Tuple{LightGraphs.SimpleGraphs.SimpleDiGraph,Int64,Array{Int64,1}}' in module 'Main'.\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "initalize_edge_counts"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initializes the edge count matrix M between the blocks. \n",
    "Calculates the new out, in and total degrees for the updated edge count matrix.\n",
    "Returns a tuple of M, d_out, d_in, d\n",
    "\"\"\"\n",
    "function initalize_edge_counts(g::DiGraph, B::Int64, b::Vector{Int64})\n",
    "    M = zeros(Int64, B, B) # create a zero matrix of B x B \n",
    "    for v in 1:nv(g)\n",
    "        for n in out_neighbors(g, v)\n",
    "            # Increment count by 1\n",
    "            # NOTE: Column major instead of row major\n",
    "            info(\"Incrementing M at ($(b[n]), $(b[v]) )\")\n",
    "            M[b[n], b[v]] += 1\n",
    "        end\n",
    "    end\n",
    "    # Sum across rows to get the outdegrees for each block\n",
    "    d_out = reshape(sum(M, 1), B)\n",
    "    # Sum across cols to get the indegrees for each block\n",
    "    d_in = reshape(sum(M, 2), B)\n",
    "    d = d_out + d_in\n",
    "    return M, d_out, d_in, d\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propose a new block assignment for the current node or block\n",
    "\n",
    "### Parameters\n",
    "    r : Int64\n",
    "            current block assignment for the node under consideration\n",
    "    neighbors_out : Array{Int64, 2}, has 2 columns.\n",
    "            out neighbors for the block\n",
    "    neighbors_in : Array{Int64, 2}, has 2 columns.\n",
    "            in neighbors for the block\n",
    "    b : Vector{Int64}\n",
    "        array of block assignment for each node\n",
    "    M : Array{Int64, 2}, size is (B, B)\n",
    "            edge count matrix between all the blocks.\n",
    "    d : Vector{Int}\n",
    "            total number of edges to and from each block\n",
    "    B : Int64\n",
    "            total number of blocks\n",
    "    agg_move : Bool\n",
    "            whether the proposal is a block move\n",
    "\n",
    "### Returns\n",
    "    s : int\n",
    "            proposed block assignment for the node under consideration\n",
    "    k_out : int\n",
    "            the out degree of the node\n",
    "    k_in : int\n",
    "            the in degree of the node\n",
    "    k : int\n",
    "            the total degree of the node\n",
    "\n",
    "### Notes\n",
    "- $d_u$: degree of block u\n",
    "\n",
    "Randomly select a neighbor of the current node, and obtain its block assignment $u$. With probability $\\frac{B}{d_u + B}$, randomly propose\n",
    "a block. Otherwise, randomly selects a neighbor to block $u$ and propose its block assignment. For block (agglomerative) moves,\n",
    "avoid proposing the current block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function propose_new_partition(\n",
    "        r::Int64, neighbors_out::Array{Int64, 2}, neighbors_in::Array{Int64, 2}, b::Vector{Int64}, M::Array{Int64, 2},\n",
    "        d::Vector{Int64}, B::Int64, agg_move::Bool\n",
    "    )\n",
    "    neighbors = vcat(neighbors_out, neighbors_in)\n",
    "    k_out = sum(neighbors_out[:, 1])\n",
    "    k_in = sum(neighbors_in[:, 1])\n",
    "    k = k_out + k_in\n",
    "    rand_neighbor = sample(neighbors[:,0], pweights(neighbors[:,1]/k))\n",
    "    u = b[rand_neighbor]\n",
    "    # propose a new block randomly\n",
    "    if rand() <= B/(d[u] + B):  # chance inversely prop. to block_degree\n",
    "        if agg_move:  # force proposal to be different from current block\n",
    "            candidates = set(1:B)\n",
    "            pop!(candidates, r)\n",
    "            s = sample(collect(candidates))\n",
    "        else:\n",
    "            s = rand(1:B)\n",
    "    else:  # propose by random draw from neighbors of block partition[rand_neighbor]\n",
    "        multinomial_prob = M[:, u] + M[u, :] / d[u]\n",
    "        if agg_move:  # force proposal to be different from current block\n",
    "            multinomial_prob[r] = 0\n",
    "            if sum(multinomial_prob) == 0:  # the current block has no neighbors. randomly propose a different block\n",
    "                candidates = set(1:B)\n",
    "                pop!(candidates, r)\n",
    "                s = sample(collect(candidates))\n",
    "                return s, k_out, k_in, k\n",
    "            else:\n",
    "                multinomial_prob = multinomial_prob / sum(multinomial_prob)\n",
    "        candidates = find(x -> x != 0, multinomial_prob)\n",
    "        \n",
    "        s = candidates[(np.random.multinomial(1, multinomial_prob[candidates])[0]]\n",
    "    return s, k_out, k_in, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91msyntax: line break in \":\" expression\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91msyntax: line break in \":\" expression\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "function main(num_vertices::Int64, base_filename::String)\n",
    "    #input_filename = '../../data/static/simulated_blockmodel_graph_500_nodes'\n",
    "    #true_partition_available = true\n",
    "    #visualize_graph = True # whether to plot the graph layout colored with intermediate partitions\n",
    "    #verbose = True # whether to print updates of the partitioning\n",
    "    \n",
    "    # Create the graph\n",
    "    g = DiGraph(num_vertices)\n",
    "    # Load the first part of the graph\n",
    "    load_graph!(g, base_filename, 1)\n",
    "    info(g, \"Loaded\")\n",
    "\n",
    "    # initialize by putting each node in its own block (N blocks)\n",
    "    num_blocks = num_vertices\n",
    "    partition = collect(1:num_vertices)\n",
    "\n",
    "    # partition update parameters\n",
    "    Î² = 3 # exploitation versus exploration (higher value favors exploitation)\n",
    "\n",
    "    # agglomerative partition update parameters\n",
    "    num_agg_proposals_per_block = 10 # number of proposals per block\n",
    "    num_block_reduction_rate = 0.5 # fraction of blocks to reduce until the golden ratio bracket is established\n",
    "\n",
    "    # nodal partition updates parameters\n",
    "    max_num_nodal_itr = 100 # maximum number of iterations\n",
    "    delta_entropy_threshold1 = 5e-4 # stop iterating when the change in entropy falls below this fraction of the overall entropy\n",
    "                                    # lowering this threshold results in more nodal update iterations and likely better performance, but longer runtime\n",
    "    delta_entropy_threshold2 = 1e-4 # threshold after the golden ratio bracket is established (typically lower to fine-tune to partition) \n",
    "    delta_entropy_moving_avg_window = 3 # width of the moving average window for the delta entropy convergence criterion\n",
    "\n",
    "    # initialize edge counts and block degrees\n",
    "    interblock_edge_count, block_degrees_out, block_degrees_in, block_degrees = \\\n",
    "        initialize_edge_counts(g, num_blocks, partition)\n",
    "\n",
    "    # initialize items before iterations to find the partition with the optimal number of blocks\n",
    "    optimal_B_found = false\n",
    "    old_b = [[], [], []]  # partition for the high, best, and low number of blocks so far\n",
    "    old_M = [[], [], []]  # edge count matrix for the high, best, and low number of blocks so far\n",
    "    old_d = [[], [], []]  # block degrees for the high, best, and low number of blocks so far\n",
    "    old_d_out = [[], [], []]  # out block degrees for the high, best, and low number of blocks so far\n",
    "    old_d_in = [[], [], []]  # in block degrees for the high, best, and low number of blocks so far\n",
    "    old_S = [Inf, Inf, Inf] # overall entropy for the high, best, and low number of blocks so far\n",
    "    old_B = [[], [], []]  # number of blocks for the high, best, and low number of blocks so far\n",
    "    \n",
    "    \n",
    "    num_blocks_to_merge = floor(Int64, num_blocks*num_block_reduction_rate)\n",
    "\n",
    "    # begin partitioning by finding the best partition with the optimal number of blocks\n",
    "    while optimal_num_blocks_found == false\n",
    "        # begin agglomerative partition updates (i.e. block merging)\n",
    "        println(\"\\nMerging down blocks from $num_blocks to $(num_blocks - num_blocks_to_merge)\")\n",
    "        \n",
    "        best_merge_for_each_block = fill(-1, num_blocks) # initialize to no merge\n",
    "        delta_entropy_for_each_block = fill(Inf, num_blocks) # initialize criterion\n",
    "        block_partition = 1:num_blocks\n",
    "        for current_block in 1:num_blocks: # evalaute agglomerative updates for each block\n",
    "            for proposal_idx in 1:num_agg_proposals_per_block:\n",
    "                # populate edges to neighboring blocks\n",
    "                out_blocks = find(x -> x != 0, interblock_edge_count[:, current_block])\n",
    "                out_blocks_counts = hcat(out_blocks, interblock_edge_count[out_blocks, current_block])\n",
    "                in_blocks = find(x -> x != 0, interblock_edge_count[current_block, :])\n",
    "                in_blocks_counts = hcat(in_blocks, interblock_edge_count[current_block, in_blocks])\n",
    "\n",
    "                # propose a new block to merge with\n",
    "                proposal, num_out_neighbor_edges, num_in_neighbor_edges, num_neighbor_edges = \\\n",
    "                propose_new_partition(current_block, out_blocks, in_blocks, block_partition, interblock_edge_count, block_degrees, num_blocks, 1, use_sparse_matrix)\n",
    "\n",
    "                # compute the two new rows and columns of the interblock edge count matrix\n",
    "                new_interblock_edge_count_current_block_row, new_interblock_edge_count_new_block_row, new_interblock_edge_count_current_block_col, new_interblock_edge_count_new_block_col = \\\n",
    "                    compute_new_rows_cols_interblock_edge_count_matrix(interblock_edge_count, current_block, proposal, out_blocks[:,0], out_blocks[:,1], in_blocks[:,0], in_blocks[:,1], interblock_edge_count[current_block, current_block], 1, use_sparse_matrix)    \n",
    "\n",
    "                # compute new block degrees           \n",
    "                block_degrees_out_new, block_degrees_in_new, block_degrees_new = compute_new_block_degrees(current_block, proposal, block_degrees_out, block_degrees_in, block_degrees, num_out_neighbor_edges, num_in_neighbor_edges, num_neighbor_edges)\n",
    "\n",
    "                # compute change in entropy / posterior\n",
    "                delta_entropy = compute_delta_entropy(current_block, proposal, interblock_edge_count, new_interblock_edge_count_current_block_row, new_interblock_edge_count_new_block_row, new_interblock_edge_count_current_block_col, new_interblock_edge_count_new_block_col, block_degrees_out, block_degrees_in, block_degrees_out_new, block_degrees_in_new, use_sparse_matrix)\n",
    "                if delta_entropy < delta_entropy_for_each_block[current_block]: # a better block candidate was found\n",
    "                    best_merge_for_each_block[current_block] = proposal\n",
    "                    delta_entropy_for_each_block[current_block] = delta_entropy\n",
    "\n",
    "        # carry out the best merges\n",
    "        partition, num_blocks = carry_out_best_merges(delta_entropy_for_each_block, best_merge_for_each_block, partition, num_blocks, num_blocks_to_merge)\n",
    "\n",
    "        # re-initialize edge counts and block degrees\n",
    "        interblock_edge_count, block_degrees_out, block_degrees_in, block_degrees = initialize_edge_counts(out_neighbors, num_blocks, partition, use_sparse_matrix)\n",
    "\n",
    "        # perform nodal partition updates\n",
    "        if verbose:\n",
    "            print(\"Beginning nodal updates\")\n",
    "        total_num_nodal_moves = 0            \n",
    "        itr_delta_entropy = np.zeros(max_num_nodal_itr)\n",
    "\n",
    "        # compute the global entropy for MCMC convergence criterion\n",
    "        overall_entropy = compute_overall_entropy(interblock_edge_count, block_degrees_out, block_degrees_in, num_blocks, N, E, use_sparse_matrix)\n",
    "\n",
    "        for itr in range(max_num_nodal_itr):\n",
    "            num_nodal_moves = 0;\n",
    "            itr_delta_entropy[itr] = 0\n",
    "\n",
    "            for current_node in range(N):\n",
    "                current_block = partition[current_node] \n",
    "                # propose a new block for this node\n",
    "                proposal, num_out_neighbor_edges, num_in_neighbor_edges, num_neighbor_edges = propose_new_partition(current_block, out_neighbors[current_node], in_neighbors[current_node], partition, interblock_edge_count, block_degrees, num_blocks, 0, use_sparse_matrix)\n",
    "\n",
    "                # determine whether to accept or reject the proposal\n",
    "                if (proposal != current_block):\n",
    "                    # compute block counts of in and out neighbors\n",
    "                    blocks_out, inverse_idx_out = np.unique(partition[out_neighbors[current_node][:,0]], return_inverse = True)\n",
    "                    count_out = np.bincount(inverse_idx_out, weights=out_neighbors[current_node][:,1]).astype(int)\n",
    "                    blocks_in, inverse_idx_in = np.unique(partition[in_neighbors[current_node][:,0]], return_inverse = True)\n",
    "                    count_in = np.bincount(inverse_idx_in, weights=in_neighbors[current_node][:,1]).astype(int)\n",
    "\n",
    "                    # compute the two new rows and columns of the interblock edge count matrix\n",
    "                    self_edge_weight = np.sum(out_neighbors[current_node][np.where(out_neighbors[current_node][:,0]==current_node),1]) # check if this node has a self edge\n",
    "                    new_interblock_edge_count_current_block_row, new_interblock_edge_count_new_block_row, new_interblock_edge_count_current_block_col, new_interblock_edge_count_new_block_col = \\\n",
    "                        compute_new_rows_cols_interblock_edge_count_matrix(interblock_edge_count, current_block, proposal, blocks_out, count_out, blocks_in, count_in, self_edge_weight, 0, use_sparse_matrix)\n",
    "\n",
    "                    # compute new block degrees           \n",
    "                    block_degrees_out_new, block_degrees_in_new, block_degrees_new = compute_new_block_degrees(current_block, proposal, block_degrees_out, block_degrees_in, block_degrees, num_out_neighbor_edges, num_in_neighbor_edges, num_neighbor_edges)\n",
    "\n",
    "                    # compute the Hastings correction\n",
    "                    Hastings_correction = compute_Hastings_correction(blocks_out, count_out, blocks_in, count_in, proposal, interblock_edge_count, new_interblock_edge_count_current_block_row, new_interblock_edge_count_current_block_col, num_blocks, block_degrees, block_degrees_new, use_sparse_matrix)\n",
    "\n",
    "                    # compute change in entropy / posterior\n",
    "                    delta_entropy = compute_delta_entropy(current_block, proposal, interblock_edge_count, new_interblock_edge_count_current_block_row, new_interblock_edge_count_new_block_row, new_interblock_edge_count_current_block_col, new_interblock_edge_count_new_block_col, block_degrees_out, block_degrees_in, block_degrees_out_new, block_degrees_in_new, use_sparse_matrix)\n",
    "\n",
    "                    # compute probability of acceptance\n",
    "                    p_accept = np.min([np.exp(-beta*delta_entropy)*Hastings_correction, 1])\n",
    "\n",
    "                    # if accept the proposal, update the partition, inter_block_edge_count, and block degrees\n",
    "                    if (np.random.uniform() <= p_accept):\n",
    "                        total_num_nodal_moves += 1\n",
    "                        num_nodal_moves += 1\n",
    "                        itr_delta_entropy[itr] += delta_entropy\n",
    "                        partition, interblock_edge_count, block_degrees_out, block_degrees_in, block_degrees = update_partition(partition, current_node, current_block, proposal, interblock_edge_count, new_interblock_edge_count_current_block_row, new_interblock_edge_count_new_block_row, new_interblock_edge_count_current_block_col, new_interblock_edge_count_new_block_col, block_degrees_out_new, block_degrees_in_new, block_degrees_new, use_sparse_matrix)\n",
    "            if verbose:\n",
    "                print(\"Itr: {}, number of nodal moves: {}, delta S: {:0.5f}\".format(itr, num_nodal_moves, itr_delta_entropy[itr]/float(overall_entropy)))\n",
    "            if itr>=(delta_entropy_moving_avg_window-1): # exit MCMC if the recent change in entropy falls below a small fraction of the overall entropy\n",
    "                if not(np.all(np.isfinite(old_overall_entropy))): # golden ratio bracket not yet established \n",
    "                    if (-np.mean(itr_delta_entropy[(itr-delta_entropy_moving_avg_window+1):itr]) < (delta_entropy_threshold1*overall_entropy)):\n",
    "                        break\n",
    "                else: # golden ratio bracket is established. Fine-tuning partition.\n",
    "                    if (-np.mean(itr_delta_entropy[(itr-delta_entropy_moving_avg_window+1):itr]) < (delta_entropy_threshold2*overall_entropy)):\n",
    "                        break\n",
    "\n",
    "        # compute the global entropy for determining the optimal number of blocks\n",
    "        overall_entropy = compute_overall_entropy(interblock_edge_count, block_degrees_out, block_degrees_in, num_blocks, N, E, use_sparse_matrix)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Total number of nodal moves: {}, overall_entropy: {:0.2f}\".format(total_num_nodal_moves, overall_entropy))\n",
    "        if visualize_graph & use_graph_tool_options:\n",
    "            graph_object = plot_graph_with_partition(out_neighbors, partition, graph_object)\n",
    "\n",
    "        # check whether the partition with optimal number of block has been found; if not, determine and prepare for the next number of blocks to try\n",
    "        partition, interblock_edge_count, block_degrees, block_degrees_out, block_degrees_in, num_blocks, num_blocks_to_merge, old_partition, old_interblock_edge_count, old_block_degrees, old_block_degrees_out, old_block_degrees_in, old_overall_entropy, old_num_blocks, optimal_num_blocks_found = \\\n",
    "            prepare_for_partition_on_next_num_blocks(overall_entropy, partition, interblock_edge_count, block_degrees, block_degrees_out, block_degrees_in, num_blocks, old_partition, old_interblock_edge_count, old_block_degrees, old_block_degrees_out, old_block_degrees_in, old_overall_entropy, old_num_blocks, num_block_reduction_rate)\n",
    "\n",
    "        if verbose:\n",
    "            print('Overall entropy: {}'.format(old_overall_entropy))\n",
    "            print('Number of blocks: {}'.format(old_num_blocks))\n",
    "            if optimal_num_blocks_found:\n",
    "                print('\\nOptimal partition found with {} blocks'.format(num_blocks))\n",
    "    if use_timeit:\n",
    "        t1 = timeit.default_timer()\n",
    "        print('\\nGraph partition took {} seconds'.format(t1-t0))\n",
    "\n",
    "    # evaluate output partition against the true partition\n",
    "    evaluate_partition(true_partition, partition)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
